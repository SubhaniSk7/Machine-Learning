{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import h5py\n",
    "import sklearn.svm\n",
    "import random\n",
    "from sklearn.metrics import f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    # constructor\n",
    "    def __init__(self, neurons=0, theta=[], b=[], z=[], a=[],delta=[],DELTA_THETA=[],DELTA_BIAS=[],dTheta=[],dBias=[]):\n",
    "        self.neurons = 0 # neurons count in layer\n",
    "        self.theta = [] # Weight vector(W)\n",
    "        self.b=[] # bias\n",
    "        self.z = [] # hypothesis z = W.T * X + b = here = theta.T * X + b\n",
    "        self.a = [] # activation function a=sigmoid(z) or relu(z) or anyother(z)\n",
    "        self.delta = [] # Loss or Error function delta= delta_cross_entropy() or anyother()\n",
    "        self.DELTA_THETA = [] # only derivative weight vector =dw\n",
    "        self.DELTA_BIAS = [] # only derivative bias vector =db\n",
    "        self.dTheta = [] # complete derivation term = (1/m)*(DELTA + (lambda*theta))\n",
    "        self.dBias = [] # complete derivation term \n",
    "\n",
    "    def setNeurons(self, neurons):\n",
    "        self.neurons = neurons\n",
    "\n",
    "    def getNeurons(self):\n",
    "        return self.neurons\n",
    "\n",
    "    def setTheta(self, theta):\n",
    "        self.theta = theta\n",
    "\n",
    "    def getTheta(self):\n",
    "        return self.theta\n",
    "\n",
    "    def setB(self, b):\n",
    "        self.b = b\n",
    "\n",
    "    def getB(self):\n",
    "        return self.b\n",
    "\n",
    "    def setZ(self, z):\n",
    "        self.z = z\n",
    "\n",
    "    def getZ(self):\n",
    "        return self.z\n",
    "\n",
    "    def setA(self, a):\n",
    "        self.a = a\n",
    "\n",
    "    def getA(self):\n",
    "        return self.a\n",
    "\n",
    "    def setDelta(self, delta):\n",
    "        self.delta = delta\n",
    "\n",
    "    def getDelta(self):\n",
    "        return self.delta\n",
    "\n",
    "    def setDELTA_THETA(self, DELTA_THETA):\n",
    "        self.DELTA_THETA = DELTA_THETA\n",
    "\n",
    "    def getDELTA_THETA(self):\n",
    "        return self.DELTA_THETA\n",
    "\n",
    "    def setDELTA_BIAS(self, DELTA_BIAS):\n",
    "        self.DELTA_BIAS = DELTA_BIAS\n",
    "\n",
    "    def getDELTA_BIAS(self):\n",
    "        return self.DELTA_BIAS\n",
    "    \n",
    "    def setDTheta(self, dTheta):\n",
    "        self.dTheta = dTheta\n",
    "\n",
    "    def getDTheta(self):\n",
    "        return self.dTheta\n",
    "    \n",
    "    def setDBias(self, dBias):\n",
    "        self.dBias = dBias\n",
    "\n",
    "    def getDBias(self):\n",
    "        return self.dBias\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = h5py.File('../MNIST_Subset-1.h5', 'r+')\n",
    "\n",
    "print(list(dataset.keys()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_X = dataset['X']\n",
    "data_Y = dataset['Y']\n",
    "\n",
    "X = np.array(data_X.value)\n",
    "Y = np.array(data_Y.value)\n",
    "\n",
    "print(X.shape,Y.shape)\n",
    "\n",
    "# Y=Y.reshape(14251,1)\n",
    "print(X.shape,Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "X=X.reshape(14251,28*28)\n",
    "print(X.shape)\n",
    "\n",
    "X = preprocessing.scale(X)\n",
    "\n",
    "m=X.shape[0]\n",
    "print('no.of samples:',m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=X.T # transposed X now shape=784 x 14251 => now each column is one datapoint\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return (1.0/(1.0+np.exp(-z)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    return np.divide(np.exp(z),np.sum(np.exp(z),axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stableSoftmax(z):\n",
    "    exps = np.exp(z - np.max(z))\n",
    "    return np.divide(exps , np.sum(exps,axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta_cross_entropy(z,y):\n",
    "    grad = (z-y)/m\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossEntropy(a,y):\n",
    "    return (-y*np.log(a))/m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_predicted):\n",
    "    y_multilabel = []\n",
    "    for p in y_predicted:\n",
    "        y_multilabel.append(list(p).index(max(p)))\n",
    "        \n",
    "    print(accuracy_score(y_multilabel, Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_actual=[] # changed to 1 at their index\n",
    "\n",
    "for i in range(Y.shape[0]):\n",
    "    temp = [0]*10\n",
    "    index = int(Y[i])\n",
    "    temp[index] = 1\n",
    "    y_actual.append(temp)\n",
    "y_actual=np.array(y_actual).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l=5\n",
    "neurons=[784,100,50,50,10]\n",
    "\n",
    "layers=[]\n",
    "for i in range(len(neurons)):\n",
    "    lay=Layer()\n",
    "    \n",
    "    if(i!=len(neurons)-1):\n",
    "        DELTA_THETA=np.zeros((neurons[i+1],neurons[i]))\n",
    "        theta=np.random.uniform(low=0.1,high=1,size=(neurons[i],neurons[i+1]))\n",
    "        #DELTA_BIAS=\n",
    "        #bias=\n",
    "        \n",
    "        lay.setDELTA_THETA(DELTA_THETA)\n",
    "        lay.setTheta(theta)\n",
    "        \n",
    "    layers.append(lay)\n",
    "\n",
    "print(len(layers))\n",
    "for i in range(l-1):# n-1 layers becz last layer don't have DELTA_THETA and theta vectors\n",
    "    print('layer',i,'-->theta:',layers[i].getTheta().shape,' DELTA:',layers[i].getDELTA_THETA().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "layers[0].setA(X)\n",
    "regParam=0.0001\n",
    "alpha=1\n",
    "maxIterations=20000\n",
    "\n",
    "for iter in range(maxIterations):\n",
    "    \n",
    "    # Forward propagation\n",
    "    for i in range(1,l):\n",
    "        z=np.dot(layers[i-1].getTheta().T, layers[i-1].getA())\n",
    "        if(i==l-1):\n",
    "            a=softmax(z)\n",
    "        else:\n",
    "            a=sigmoid(z)\n",
    "        layers[i].setZ(z)\n",
    "        layers[i].setA(a)\n",
    "    \n",
    "    # Backward Propagation\n",
    "    for i in range(l-1,-1,-1):\n",
    "        loss=None\n",
    "        if(i==l-1):\n",
    "            loss=delta_cross_entropy(layers[i].getA(),y_actual)\n",
    "        else:\n",
    "            loss=np.dot(layers[i].getTheta(),layers[i+1].getDelta()) * (layers[i].getA()*(1-layers[i].getA()))\n",
    "        layers[i].setDelta(loss)\n",
    "    \n",
    "    for i in range(0,l-1):\n",
    "        D=layers[i].getDELTA_THETA() + np.dot(layers[i+1].getDelta(),layers[i].getA().T)\n",
    "        layers[i].setDELTA_THETA(D)\n",
    "    \n",
    "    for i in range(0,l-1):\n",
    "        dT=(1/m)*(layers[i].getDELTA_THETA().T+(regParam*layers[i].getTheta()))\n",
    "        layers[i].setDTheta(dT)\n",
    "    \n",
    "    print('Iteration:',iter,'--> ',end='')\n",
    "    accuracy(layers[-1].getA().T)\n",
    "    if(accuracy(layers[-1].getA().T) == np.nan):\n",
    "        break\n",
    "    \n",
    "    for i in range(0,l-1):\n",
    "        newTh=layers[i].getTheta()-(alpha*layers[i].getDTheta())\n",
    "        layers[i].setTheta(newTh)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# DELTA1=np.zeros((100,784))\n",
    "# DELTA2=np.zeros((10,100))\n",
    "\n",
    "# theta1=np.random.uniform(low=0.1,high=1,size=(784,100)) #randomly generated weight vector for Hidden layer 1 and Size is 78400= 784featues and 100 neurons in hidden layer1\n",
    "# theta2=np.random.uniform(low=0.1,high=1,size=(100,10)) #randomly generated weight vector for output layer and Size is 1000= 100Hidden units and 10 output layer units in hidden layer1\n",
    "\n",
    "# DELTA1=np.zeros((784,100))\n",
    "# DELTA2=np.zeros((100,10))\n",
    "\n",
    "# # Layer 1 (Input Layer)\n",
    "# a1=X\n",
    "# regParam=0.0001\n",
    "# alpha=1\n",
    "# maxIterations=1\n",
    "# for iter in range(maxIterations):\n",
    "    \n",
    "#     # ForwardProp\n",
    "#     # Layer 2 (Hidden layer 1)\n",
    "#     z1=np.dot(theta1.T,a1)\n",
    "#     a2=sigmoid(z1)\n",
    "    \n",
    "#     # Layer 3 (Output layer)\n",
    "#     z2=np.dot(theta2.T,a2)\n",
    "# #     a3=softmax(z2)\n",
    "#     a3=stableSoftmax(z2)\n",
    "\n",
    "#     yPredicted=a3\n",
    "    \n",
    "#     #BackProp\n",
    "#     delta3=delta_cross_entropy(a3,y_actual)\n",
    "#     delta2=np.dot(theta2,delta3) * (a2*(1-a2))\n",
    "    \n",
    "    \n",
    "#     DELTA2=DELTA2+np.dot(delta3,a2.T) #only derivative\n",
    "#     DELTA1=DELTA1+np.dot(delta2,a1.T) #only derivative\n",
    "    \n",
    "#     print(DELTA1.shape,theta1.shape)\n",
    "#     print(DELTA2.shape,theta2.shape)\n",
    "#     dTheta2=(1/m)*(DELTA2.T+(regParam*theta2)) #whole derivative part\n",
    "#     dTheta1=(1/m)*(DELTA1.T+(regParam*theta1)) #whole derivative part\n",
    "    \n",
    "#     print('Iteration:',iter,'--> ',end='')\n",
    "#     accuracy(yPredicted.T)\n",
    "#     if(accuracy(yPredicted.T) == np.nan):\n",
    "#         break\n",
    "        \n",
    "#     theta2=theta2-(alpha*dTheta2)\n",
    "#     theta1=theta1-(alpha*dTheta1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(testX):\n",
    "    # Forward propagation\n",
    "    for i in range(1,l):\n",
    "        z=np.dot(layers[i-1].getTheta().T, layers[i-1].getA())\n",
    "        if(i==l-1):\n",
    "            a=softmax(z)\n",
    "        else:\n",
    "            a=sigmoid(z)\n",
    "        layers[i].setZ(z)\n",
    "        layers[i].setA(a)\n",
    "\n",
    "    accuracy(layers[-1].getA().T)\n",
    "    if(accuracy(layers[-1].getA().T) == np.nan):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
